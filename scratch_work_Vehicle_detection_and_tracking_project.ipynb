{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/blob/main/scratch_work_Vehicle_detection_and_tracking_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNsHJxVARVmr"
      },
      "source": [
        "#INTRODUCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eyWwo1YRsao"
      },
      "source": [
        "We use Kalman filter for tracking objects. Kalman filter has the following important features that tracking can benefit from:\n",
        "\n",
        "Prediction of object's future location Correction of the prediction based on new measurements Reduction of noise introduced by inaccurate detections Facilitating the process of association of multiple objects to their tracks Kalman filter consists of two steps: prediction and update. The first step uses previous states to predict the current state. The second step uses the current measurement, such as detection bounding box location , to correct the state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDxajdBRzY7"
      },
      "source": [
        "#DATA SOURCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kU3AXv7SGSl"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Originial video: \n",
        "\n",
        "https://github.com/xuanzl29/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/blob/main/project_video.mp4\n",
        "\n",
        "Video captured into frames:\n",
        "\n",
        "https://github.com/xuanzl29/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/tree/main/test_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ObMsfZSXVJ"
      },
      "source": [
        "#EQUATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6f5vAYQSZ-N"
      },
      "source": [
        "### Notations:\n",
        "\n",
        "X - State Mean\n",
        "\n",
        "P - State Covariance\n",
        "\n",
        "F - State Transition Matrix\n",
        "\n",
        "Q - Process Covariance\n",
        "\n",
        "B - Control Function\n",
        "\n",
        "u - Control Input\n",
        "\n",
        "The coordinates of the upper right, upper left, bottom right, bottom left corners of the bounding box are $(x_1, y_1)$, $(x_2, y_2)$, $(x_3, y_3)$ and $(x_4, y_4)$ respectively. Since $x_1$ must equals to $x_4$, $x_2$ must equals to $x_3$, $y_1$ must equals to $y_2$, and $y_3$ equals to $y_4$, the coordinates can be written as $(x_1, y_1)$, $(x_2, y_1)$, $(x_2, y_3)$ and $(x_1, y_3)$. Changing the notation by letting $y_2$ denotes $y_3$, we have $(x_1, y_1)$, $(x_2, y_1)$, $(x_2, y_2)$ and $(x_1, y_2)$ for upper right, upper left, bottom right, bottom left respectively.\n",
        "\n",
        "\n",
        "### Prediction:\n",
        "\n",
        "By the assumption that velocity is constant so that accelaration is zero, we write out the following equations for each corner (state quation, control equation):\n",
        "\n",
        "$(x_1, y_1) = (x_{1, k-1} + dx_{1, k-1} \\Delta t, \\quad y_{1, k-1} + dy_{1, k-1})$\n",
        "\n",
        "$(x_2, y_1) = (x_{2,k-1} + dx_{2,k-1} \\Delta t, \\quad  y_{1,k-1} + dy_{1, k-1})$\n",
        "\n",
        "$(x_2, y_2) = (x_{2,k-1} + dx_{2,k-1} \\Delta t, \\quad y_{2,k-1} + dy_{2, k-1})$\n",
        "\n",
        "$(x_1, y_2) = (x_{1,k-1} + dx_{1,k-1} \\Delta t,  \\quad y_{2,k-1} + dy_{2, k-1})$.\n",
        "\n",
        "The state equation is $x_{k} = Fx + Bu$, where the state vector has eight elements as follows: $\\begin{bmatrix} y_1 \\\\ dy_1 \\\\ x_1 \\\\ dx_1 \\\\ y_2 \\\\ dy_2 \\\\ x_2 \\\\dx_2\\end{bmatrix}$, and the prediction equation is $\\bar{P} = FPF^T + Q$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "From which We have, $F = \\begin{bmatrix} \n",
        "  1 &\\Delta t &0 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &1 &0 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &1 &\\Delta t &0 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &1 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &1 &\\Delta t &0 &0 \\\\\n",
        "  0 &0 &0 &0 &0 &1 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &0 &0 &1 &\\Delta t \\\\\n",
        "  0 &0 &0 &0 &0 &0 &0 &1\n",
        "  \\end{bmatrix}$,\n",
        "  $x = \\begin{bmatrix}\n",
        "y_{1 k-1} \\\\\n",
        "dy_{1 k-1} \\\\\n",
        "x_{1 k-1} \\\\\n",
        "dx_{1 k-1} \\\\\n",
        "y_{2 k-1} \\\\\n",
        "dy_{2 k-1} \\\\\n",
        "x_{2 k-1} \\\\\n",
        "dx_{2 k-1} \\\\\n",
        " \\end{bmatrix}$,\n",
        "\n",
        " $Fx = \\begin{bmatrix}\n",
        "  y_{1 k-1} + dy_{1 k-1} \\Delta t \\\\\n",
        "  dy_{1 k-1} \\\\\n",
        "  x_{1 k-1} + dx_{1 k-1} \\Delta t \\\\\n",
        "  x_{1 k-1} \\\\\n",
        "  y_{2 k-1} + dy_{2 k-1} \\Delta t \\\\\n",
        "  y_{2 k-1} \\\\\n",
        "  x_{2 k-1} + dx_{2 k-1} \\Delta t \\\\\n",
        "  x_{2 k-1}\n",
        "  \\end{bmatrix}$,\n",
        "\n",
        "$B = \\begin{bmatrix} \n",
        "  1 &0 &0 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &1 &0 &0 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &1 &0 &0 &0 \\\\\n",
        "  0 &0 &0 &0 &0 &0 &1 &0 \\\\\n",
        "  \\end{bmatrix}$,\n",
        "  \n",
        "  $u = \\begin{bmatrix}y_{1 k-1} \\\\\n",
        "dy_{1 k-1} \\\\\n",
        "x_{1 k-1} \\\\\n",
        "dx_{1 k-1} \\\\\n",
        "y_{2 k-1} \\\\\n",
        "dy_{2 k-1} \\\\\n",
        "x_{2 k-1} \\\\\n",
        "dx_{2 k-1} \\\\\n",
        "  \\end{bmatrix}$,\n",
        "  \n",
        "  $ Bu = \\begin{bmatrix} y_{1 k-1} \\\\ x_{1 k-1} \\\\ y_{2 k-1} \\\\x_{2 k-1}\\end{bmatrix}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPxqGdzKVB1b"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# INSTALLING THE REQUIRED PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OERBr-xAQQry",
        "outputId": "969b0ff0-97bb-4c4a-8af5-9f31d091720a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.7/dist-packages (2.5.3)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.11.1+cu111)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai) (2.23.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai) (21.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai) (21.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai) (0.0.5)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai) (2.2.4)\n",
            "Requirement already satisfied: fastcore<1.4,>=1.3.22 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.29)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai) (1.3.5)\n",
            "Requirement already satisfied: torch<1.11,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.10.0+cu111)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.21.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (4.63.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai) (1.1.0)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.7/dist-packages (0.11.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->matplotlib-venn) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "#installing packages\n",
        "!pip install fastai\n",
        "!pip install -Uqq fastbook\n",
        "!pip install matplotlib-venn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVPsFDLbdKW5"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import cv2\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate, accuracy\n",
        "import fastai\n",
        "\n",
        "from fastai.vision.all import *\n",
        "\n",
        "import fastbook\n",
        "#fastbook.setup_book()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "from fastai.vision import *\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate, accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "from fastai.vision.all import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EMbQlj7OuKc"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ5PD-gjPB8a"
      },
      "outputs": [],
      "source": [
        "path = 'https://github.com/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/tree/main/Videos%20Frame'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ1sjXhziWpw"
      },
      "outputs": [],
      "source": [
        "#import os\n",
        "\n",
        "#list = os.listdir(path)\n",
        "#number_files = len(list)\n",
        "#print (number_files)                   # Total 23 images are present in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f6aGE41Eym5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "92bf8a5a-ea79-4ce7-db2a-fc91dff53cf9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/https:/github.com/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/tree/main/Videos%20Frame'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "cwd = (os.path.realpath('https://github.com/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/tree/main/Videos%20Frame'))\n",
        "cwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkLS9IL7i3tp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "20dc8cb3-8213-478d-baba-344c7f30110f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-26058dc390cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnumber_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumber_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://github.com/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/tree/main/Videos%20Frame'"
          ]
        }
      ],
      "source": [
        "list = os.listdir(path)                  \n",
        "number_files = len(list)\n",
        "print (number_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvX-ZlFCfiE"
      },
      "source": [
        "# Implementation and test car detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2Hp5HlQCa74"
      },
      "source": [
        "In the pipeline, vehicle (car) detection takes a captured image as input and produces the bounding boxes as the output. We use TensorFlow Object Detection API, which is an open source framework built on top of TensorFlow to construct, train and deploy object detection models. The Object Detection API also comes with a collection of detection models pre-trained on the COCO dataset that are well suited for fast prototyping. Specifically, we use a lightweight model: ssd_mobilenet_v1_coco that is based on Single Shot Multibox Detection (SSD) framework with minimal modification. Though this is a general-purpose detection model (not specifically optimized for vehicle detection), we find this model achieves the balance between bounding box accuracy and inference time.\n",
        "\n",
        "The detector is implemented in CarDetector class in detector.py. The output are the coordinates of the bounding boxes (in the format of [y_up, x_left, y_down, x_right] ) of all the detected vehicles.\n",
        "\n",
        "The COCO dataset contains images of 90 classes, with the first 14 classes all related to transportation, including bicycle, car, and bus, etc. The ID for car is 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haAE75TBk5b5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "cwd = os.path.dirname(os.path.realpath('https://github.com/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/blob/main/Videos%20Frame/frame001.jpg'))\n",
        "\n",
        "class CarDetector(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        self.car_boxes = []\n",
        "        \n",
        "        os.chdir(cwd)\n",
        "        \n",
        "        #Tensorflow localization/detection model\n",
        "        # Single-shot-dectection with mobile net architecture trained on COCO dataset\n",
        "        \n",
        "        detect_model_name = 'https://github.com/Karthika-ai/Vehicle-Detection-and-Tracking-Using-Kalman-Filter/blob/main/ssd_mobilenet'\n",
        "\n",
        "        PATH_TO_CKPT = detect_model_name + '/frozen_inference_graph.pb'\n",
        "        \n",
        "        # setup tensorflow graph\n",
        "        self.detection_graph = tf.Graph()\n",
        "        \n",
        "        # configuration for possible GPU use\n",
        "        config = tf.compat.v1.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        # load frozen tensorflow detection model and initialize \n",
        "        # the tensorflow graph\n",
        "        with self.detection_graph.as_default():\n",
        "          od_graph_def = tf.compat.v1.GraphDef()\n",
        "          with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "            serialized_graph = fid.read()\n",
        "            od_graph_def.ParseFromString(serialized_graph)\n",
        "            tf.import_graph_def(od_graph_def, name='')\n",
        "               \n",
        "            self.sess = tf.compat.v1.Session(graph=self.detection_graph, config=config)\n",
        "            self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "              # Each box represents a part of the image where a particular object was detected.\n",
        "            self.boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "              # Each score represent how level of confidence for each of the objects.\n",
        "              # Score is shown on the result image, together with the class label.\n",
        "            self.scores =self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            self.classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            self.num_detections =self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "    \n",
        "    # Helper function to convert image into numpy array    \n",
        "    def load_image_into_numpy_array(self, image):\n",
        "         (im_width, im_height) = image.size\n",
        "         return np.array(image.getdata()).reshape(\n",
        "            (im_height, im_width, 3)).astype(np.uint8)       \n",
        "    # Helper function to convert normalized box coordinates to pixels\n",
        "    def box_normal_to_pixel(self, box, dim):\n",
        "    \n",
        "        height, width = dim[0], dim[1]\n",
        "        box_pixel = [int(box[0]*height), int(box[1]*width), int(box[2]*height), int(box[3]*width)]\n",
        "        return np.array(box_pixel)       \n",
        "        \n",
        "    def get_localization(self, image, visual=False):  \n",
        "        \n",
        "        \"\"\"Determines the locations of the cars in the image\n",
        "\n",
        "        Args:\n",
        "            image: camera image\n",
        "\n",
        "        Returns:\n",
        "            list of bounding boxes: coordinates [y_up, x_left, y_down, x_right]\n",
        "\n",
        "        \"\"\"\n",
        "        category_index={1: {'id': 1, 'name': u'person'},\n",
        "                        2: {'id': 2, 'name': u'bicycle'},\n",
        "                        3: {'id': 3, 'name': u'car'},\n",
        "                        4: {'id': 4, 'name': u'motorcycle'},\n",
        "                        5: {'id': 5, 'name': u'airplane'},\n",
        "                        6: {'id': 6, 'name': u'bus'},\n",
        "                        7: {'id': 7, 'name': u'train'},\n",
        "                        8: {'id': 8, 'name': u'truck'},\n",
        "                        9: {'id': 9, 'name': u'boat'},\n",
        "                        10: {'id': 10, 'name': u'traffic light'},\n",
        "                        11: {'id': 11, 'name': u'fire hydrant'},\n",
        "                        13: {'id': 13, 'name': u'stop sign'},\n",
        "                        14: {'id': 14, 'name': u'parking meter'}}  \n",
        "        \n",
        "        # The following code snippet implements the actual detection using TensorFlow API.\n",
        "\n",
        "        with self.detection_graph.as_default():\n",
        "              image_expanded = np.expand_dims(image, axis=0)\n",
        "              (boxes, scores, classes, num_detections) = self.sess.run(\n",
        "                  [self.boxes, self.scores, self.classes, self.num_detections],\n",
        "                  feed_dict={self.image_tensor: image_expanded})\n",
        "\n",
        "        # Here boxes, scores, and classes represent the bounding box, confidence level, and class name corresponding to each of the detection, respectively. \n",
        "        # Next, we select the detections that are cars and have a confidence greater than a threshold ( e.g., 0.3 in this case).\n",
        "\n",
        "\n",
        "              if visual == True:\n",
        "                  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "                      image,\n",
        "                      np.squeeze(boxes),\n",
        "                      np.squeeze(classes).astype(np.int32),\n",
        "                      np.squeeze(scores),\n",
        "                      category_index,\n",
        "                      use_normalized_coordinates=True,min_score_thresh=.4,\n",
        "                      line_thickness=3)\n",
        "    \n",
        "                  plt.figure(figsize=(9,6))\n",
        "                  plt.imshow(image)\n",
        "                  plt.show()  \n",
        "              \n",
        "              boxes=np.squeeze(boxes)\n",
        "              classes =np.squeeze(classes)\n",
        "              scores = np.squeeze(scores)\n",
        "    \n",
        "              cls = classes.tolist()\n",
        "              \n",
        "              # The ID for car in COCO data set is 3 \n",
        "              idx_vec = [i for i, v in enumerate(cls) if ((v==3) and (scores[i]>0.3))]\n",
        "              \n",
        "              if len(idx_vec) ==0:\n",
        "                  print('no detection!')\n",
        "                  self.car_boxes = []  \n",
        "              else:\n",
        "                  tmp_car_boxes=[]\n",
        "                  for idx in idx_vec:\n",
        "                      dim = image.shape[0:2]\n",
        "                      box = self.box_normal_to_pixel(boxes[idx], dim)\n",
        "                      box_h = box[2] - box[0]\n",
        "                      box_w = box[3] - box[1]\n",
        "                      ratio = box_h/(box_w + 0.01)\n",
        "\n",
        "\n",
        "                      #To further reduce possible false positives, we include thresholds for bounding box width, height, and height-to-width ratio.\n",
        "                      \n",
        "                      if ((ratio < 0.8) and (box_h>20) and (box_w>20)):\n",
        "                          tmp_car_boxes.append(box)\n",
        "                          print(box, ', confidence: ', scores[idx], 'ratio:', ratio)\n",
        "                         \n",
        "                      else:\n",
        "                          print('wrong ratio or wrong size, ', box, ', confidence: ', scores[idx], 'ratio:', ratio)\n",
        "                          \n",
        "                          \n",
        "                  \n",
        "                  self.car_boxes = tmp_car_boxes\n",
        "             \n",
        "        return self.car_boxes\n",
        "        \n",
        "if __name__ == '__main__':\n",
        "        # Test the performance of the detector\n",
        "        det =CarDetector()\n",
        "        os.chdir(cwd)\n",
        "        TEST_IMAGE_PATHS= glob(os.path.join('test_images/', '*.jpg'))\n",
        "        \n",
        "        for i, image_path in enumerate(TEST_IMAGE_PATHS[0:2]):\n",
        "            print('')\n",
        "            print('*************************************************')\n",
        "            \n",
        "            img_full = Image.open(image_path)\n",
        "            img_full_np = det.load_image_into_numpy_array(img_full)\n",
        "            img_full_np_copy = np.copy(img_full_np)\n",
        "            start = time.time()\n",
        "            b = det.get_localization(img_full_np, visual=False)\n",
        "            end = time.time()\n",
        "            print('Localization time: ', end-start)\n",
        "#            \n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdRw8XSV2TpO"
      },
      "source": [
        "# Helper classes and functions for detection and tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q37owMD2fIs"
      },
      "outputs": [],
      "source": [
        "class Box:\n",
        "    def __init__(self):\n",
        "        self.x, self.y = float(), float()\n",
        "        self.w, self.h = float(), float()\n",
        "        self.c = float()\n",
        "        self.prob = float()\n",
        "\n",
        "def overlap(x1,w1,x2,w2):\n",
        "    l1 = x1 - w1 / 2.;\n",
        "    l2 = x2 - w2 / 2.;\n",
        "    left = max(l1, l2)\n",
        "    r1 = x1 + w1 / 2.;\n",
        "    r2 = x2 + w2 / 2.;\n",
        "    right = min(r1, r2)\n",
        "    return right - left;\n",
        "\n",
        "def box_intersection(a, b):\n",
        "    w = overlap(a.x, a.w, b.x, b.w);\n",
        "    h = overlap(a.y, a.h, b.y, b.h);\n",
        "    if w < 0 or h < 0: return 0;\n",
        "    area = w * h;\n",
        "    return area;\n",
        "\n",
        "def box_union(a, b):\n",
        "    i = box_intersection(a, b);\n",
        "    u = a.w * a.h + b.w * b.h - i;\n",
        "    return u;\n",
        "\n",
        "def box_iou(a, b):\n",
        "    return box_intersection(a, b) / box_union(a, b);\n",
        "\n",
        "def box_iou2(a, b):\n",
        "    '''\n",
        "    Helper funciton to calculate the ratio between intersection and the union of\n",
        "    two boxes a and b\n",
        "    a[0], a[1], a[2], a[3] <-> left, up, right, bottom\n",
        "    '''\n",
        "    \n",
        "    w_intsec = np.maximum (0, (np.minimum(a[2], b[2]) - np.maximum(a[0], b[0])))\n",
        "    h_intsec = np.maximum (0, (np.minimum(a[3], b[3]) - np.maximum(a[1], b[1])))\n",
        "    s_intsec = w_intsec * h_intsec\n",
        "    s_a = (a[2] - a[0])*(a[3] - a[1])\n",
        "    s_b = (b[2] - b[0])*(b[3] - b[1])\n",
        "  \n",
        "    return float(s_intsec)/(s_a + s_b -s_intsec)\n",
        "\n",
        "def convert_to_pixel(box_yolo, img, crop_range):\n",
        "    '''\n",
        "    Helper function to convert (scaled) coordinates of a bounding box \n",
        "    to pixel coordinates. \n",
        "    \n",
        "    Example (0.89361443264143803, 0.4880486045564924, 0.23544462956491041, \n",
        "    0.36866588651069609)\n",
        "    \n",
        "    crop_range: specifies the part of image to be cropped\n",
        "    '''\n",
        "    \n",
        "    box = box_yolo\n",
        "    imgcv = img\n",
        "    [xmin, xmax] = crop_range[0]\n",
        "    [ymin, ymax] = crop_range[1]\n",
        "    h, w, _ = imgcv.shape\n",
        "    \n",
        "    # Calculate left, top, width, and height of the bounding box\n",
        "    left = int((box.x - box.w/2.)*(xmax - xmin) + xmin)\n",
        "    top = int((box.y - box.h/2.)*(ymax - ymin) + ymin)\n",
        "    \n",
        "    width = int(box.w*(xmax - xmin))\n",
        "    height = int(box.h*(ymax - ymin))\n",
        "    \n",
        "    # Deal with corner cases\n",
        "    if left  < 0    :  left = 0\n",
        "    if top   < 0    :   top = 0\n",
        "    \n",
        "    # Return the coordinates (in the unit of the pixels)\n",
        "  \n",
        "    box_pixel = np.array([left, top, width, height])\n",
        "    return box_pixel\n",
        "\n",
        "\n",
        "\n",
        "def convert_to_cv2bbox(bbox, img_dim = (1280, 720)):\n",
        "    '''\n",
        "    Helper fucntion for converting bbox to bbox_cv2\n",
        "    bbox = [left, top, width, height]\n",
        "    bbox_cv2 = [left, top, right, bottom]\n",
        "    img_dim: dimension of the image, img_dim[0]<-> x\n",
        "    img_dim[1]<-> y\n",
        "    '''\n",
        "    left = np.maximum(0, bbox[0])\n",
        "    top = np.maximum(0, bbox[1])\n",
        "    right = np.minimum(img_dim[0], bbox[0] + bbox[2])\n",
        "    bottom = np.minimum(img_dim[1], bbox[1] + bbox[3])\n",
        "    \n",
        "    return (left, top, right, bottom)\n",
        "    \n",
        "    \n",
        "def draw_box_label(img, bbox_cv2, box_color=(0, 255, 255), show_label=True):\n",
        "    '''\n",
        "    Helper funciton for drawing the bounding boxes and the labels\n",
        "    bbox_cv2 = [left, top, right, bottom]\n",
        "    '''\n",
        "    #box_color= (0, 255, 255)\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_size = 0.7\n",
        "    font_color = (0, 0, 0)\n",
        "    left, top, right, bottom = bbox_cv2[1], bbox_cv2[0], bbox_cv2[3], bbox_cv2[2]\n",
        "    \n",
        "    # Draw the bounding box\n",
        "    cv2.rectangle(img, (left, top), (right, bottom), box_color, 4)\n",
        "    \n",
        "    if show_label:\n",
        "        # Draw a filled box on top of the bounding box (as the background for the labels)\n",
        "        cv2.rectangle(img, (left-2, top-45), (right+2, top), box_color, -1, 1)\n",
        "        \n",
        "        # Output the labels that show the x and y coordinates of the bounding box center.\n",
        "        text_x= 'x='+str((left+right)/2)\n",
        "        cv2.putText(img,text_x,(left,top-25), font, font_size, font_color, 1, cv2.LINE_AA)\n",
        "        text_y= 'y='+str((top+bottom)/2)\n",
        "        cv2.putText(img,text_y,(left,top-5), font, font_size, font_color, 1, cv2.LINE_AA)\n",
        "    \n",
        "    return img    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoFPh2ayDLJr"
      },
      "source": [
        "# Implementing and test tracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7HZ-CIIDOI9"
      },
      "source": [
        "Kalman filter consists of two steps: prediction and update. The first step uses previous states to predict the current state. The second step uses the current measurement, such as detection bounding box location , to correct the state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1J-7WhfDcE2"
      },
      "source": [
        "The state vector has eight elements as follows:\n",
        "\n",
        "[up, up_dot, left, left_dot, down, down_dot, right, right_dot]\n",
        "\n",
        "That is, we use the coordinates and their first-order derivatives of the up left corner and lower right corner of the bounding box.\n",
        "\n",
        "Here, we are assuming the constant velocity (thus no acceleration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShBeLsDx2rZ8"
      },
      "outputs": [],
      "source": [
        "from numpy import dot\n",
        "from scipy.linalg import inv, block_diag\n",
        "import glob\n",
        "\n",
        "\n",
        "class Tracker():                                                                  # class for Kalman Filter-based tracker\n",
        "    def __init__(self):\n",
        "        # Initialize parametes for tracker (history)\n",
        "        self.id = 0                                                               # tracker's id \n",
        "        self.box = []                                                             # list to store the coordinates for a bounding box \n",
        "        self.hits = 0                                                             # number of detection matches\n",
        "        self.no_losses = 0                                                        # number of unmatched tracks (track loss)\n",
        "        \n",
        "        # Initialize parameters for Kalman Filtering\n",
        "        # The state is the (x, y) coordinates of the detection box\n",
        "        # state: [up, up_dot, left, left_dot, down, down_dot, right, right_dot]\n",
        "        # or[up, up_dot, left, left_dot, height, height_dot, width, width_dot]\n",
        "        self.x_state=[] \n",
        "        self.dt = 1.   # time interval\n",
        "        \n",
        "        # Process matrix, assuming constant velocity model\n",
        "        self.F = np.array([[1, self.dt, 0,  0,  0,  0,  0, 0],\n",
        "                           [0, 1,  0,  0,  0,  0,  0, 0],\n",
        "                           [0, 0,  1,  self.dt, 0,  0,  0, 0],\n",
        "                           [0, 0,  0,  1,  0,  0,  0, 0],\n",
        "                           [0, 0,  0,  0,  1,  self.dt, 0, 0],\n",
        "                           [0, 0,  0,  0,  0,  1,  0, 0],\n",
        "                           [0, 0,  0,  0,  0,  0,  1, self.dt],\n",
        "                           [0, 0,  0,  0,  0,  0,  0,  1]])\n",
        "        \n",
        "        \n",
        "        # Measurement matrix, assuming we can only measure the coordinates\n",
        "        \n",
        "        self.H = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
        "                           [0, 0, 1, 0, 0, 0, 0, 0],\n",
        "                           [0, 0, 0, 0, 1, 0, 0, 0], \n",
        "                           [0, 0, 0, 0, 0, 0, 1, 0]])\n",
        "        \n",
        "        \n",
        "        # Initialize the state covariance\n",
        "        self.L = 10.0\n",
        "        self.P = np.diag(self.L*np.ones(8))\n",
        "        \n",
        "        \n",
        "        # Initialize the process covariance\n",
        "        self.Q_comp_mat = np.array([[self.dt**4/4., self.dt**3/2.],\n",
        "                                    [self.dt**3/2., self.dt**2]])\n",
        "        self.Q = block_diag(self.Q_comp_mat, self.Q_comp_mat, \n",
        "                            self.Q_comp_mat, self.Q_comp_mat)\n",
        "        \n",
        "        # Initialize the measurement covariance\n",
        "        self.R_scaler = 1.0\n",
        "        self.R_diag_array = self.R_scaler * np.array([self.L, self.L, self.L, self.L])\n",
        "        self.R = np.diag(self.R_diag_array)\n",
        "\n",
        "\n",
        "        # Here self.R_scaler represents the \"magnitude\" of measurement noise relative to state noise. \n",
        "        # A low self.R_scaler indicates a more reliable measurement. \n",
        "        \n",
        "        \n",
        "    def update_R(self):   \n",
        "        R_diag_array = self.R_scaler * np.array([self.L, self.L, self.L, self.L])\n",
        "        self.R = np.diag(R_diag_array)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    def kalman_filter(self, z): \n",
        "        '''\n",
        "        Implement the Kalman Filter, including the predict and the update stages,\n",
        "        with the measurement z\n",
        "        '''\n",
        "        x = self.x_state\n",
        "        # Predict\n",
        "        x = dot(self.F, x)\n",
        "        self.P = dot(self.F, self.P).dot(self.F.T) + self.Q\n",
        "\n",
        "        #Update\n",
        "        S = dot(self.H, self.P).dot(self.H.T) + self.R\n",
        "        K = dot(self.P, self.H.T).dot(inv(S)) # Kalman gain\n",
        "        y = z - dot(self.H, x) # residual\n",
        "        x += dot(K, y)\n",
        "        self.P = self.P - dot(K, self.H).dot(self.P)\n",
        "        self.x_state = x.astype(int) # convert to integer coordinates \n",
        "                                     #(pixel values)\n",
        "        \n",
        "    def predict_only(self):  \n",
        "        '''\n",
        "        Implment only the predict stage. This is used for unmatched detections and \n",
        "        unmatched tracks\n",
        "        '''\n",
        "        x = self.x_state\n",
        "        # Predict\n",
        "        x = dot(self.F, x)\n",
        "        self.P = dot(self.F, self.P).dot(self.F.T) + self.Q\n",
        "        self.x_state = x.astype(int)\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \n",
        "    # Creat an instance\n",
        "    trk = Tracker() \n",
        "    # Test R_ratio   \n",
        "    trk.R_scaler = 1.0/16\n",
        "    # Update measurement noise covariance matrix\n",
        "    trk.update_R()\n",
        "    # Initial state\n",
        "    x_init = np.array([390, 0, 1050, 0, 513, 0, 1278, 0])\n",
        "    x_init_box = [x_init[0], x_init[2], x_init[4], x_init[6]]\n",
        "    # Measurement\n",
        "    z=np.array([399, 1022, 504, 1256])\n",
        "    trk.x_state= x_init.T\n",
        "    trk.kalman_filter(z.T)\n",
        "    # Updated state\n",
        "    x_update =trk.x_state\n",
        "    x_updated_box = [x_update[0], x_update[2], x_update[4], x_update[6]]\n",
        "    \n",
        "    print('The initial state is: ', x_init)\n",
        "    print('The measurement is: ', z)\n",
        "    print('The update state is: ', x_update)\n",
        "    \n",
        "    # Visualize the Kalman filter process and the \n",
        "    # impact of measurement nosie convariance matrix\n",
        "\n",
        "    images = [plt.imread(file) for file in glob.glob('/content/drive/MyDrive/Colab Notebooks/KALMAN FILTER PROJECT/Klaman filter images/*.jpg')]\n",
        "    \n",
        "\n",
        "    img=images[3]\n",
        "\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    #helpers.draw_box_label(img, x_init_box, box_color=(0, 255, 0))\n",
        "    draw_box_label(img, x_init_box, box_color=(0, 255, 0))\n",
        "    ax = plt.subplot(3, 1, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Initial: '+str(x_init_box))\n",
        "    \n",
        "    #helpers.draw_box_label(img, z, box_color=(255, 0, 0))\n",
        "    draw_box_label(img, z, box_color=(255, 0, 0))\n",
        "    ax = plt.subplot(3, 1, 2)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Measurement: '+str(z))\n",
        "    \n",
        "    #helpers.draw_box_label(img, x_updated_box)\n",
        "    draw_box_label(img, x_updated_box)\n",
        "    ax = plt.subplot(3, 1, 3)\n",
        "    plt.imshow(img)\n",
        "    plt.title('Updated: '+str(x_updated_box))\n",
        "    plt.show()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEN8JMpOETcF"
      },
      "source": [
        "From the above output images, we can visualize the impact of measurement noise to the Kalman filter process. \n",
        "\n",
        "The green bounding box represents the prediction (initial) state. The red bounding box represents the measurement. \n",
        "\n",
        "If measurement noise is low, the updated state (aqua colored bounding box) is very close to the measurement (aqua bounding box completely overlaps over the red bounding box).\n",
        "\n",
        "In contrast, if measurement noise is high, the updated state is very close to the initial prediction (aqua bounding box completely overlaps over the green bounding box).\n",
        "\n",
        "From the output images, there is no complete overlapping of green and aqua bounding box. Which infers that the measurement noise is relatively lower.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMp5xtjFFRYr"
      },
      "source": [
        "# ***MAIN functions***\n",
        "\n",
        "Implements the detection and tracking pipeline, including detection-track assignment and track management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptAZHPsREyEI"
      },
      "source": [
        "In the below logic, the module assign_detections_to_trackers(trackers, detections, iou_thrd = 0.3) takes from current list of trackers and new detections, output matched detections, unmatched trackers, unmatched detections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOADcePUFDsL"
      },
      "source": [
        "We implement Linear Assignment and Hungarian (Munkres) algorithm in this anlysis:\n",
        "\n",
        "If there are multiple detections, we need to match (assign) each of them to a tracker. We use intersection over union (IOU) of a tracker bounding box and detection bounding box as a metric. We solve the maximizing the sum of IOU assignment problem using the Hungarian algorithm (also known as Munkres algorithm). The machine learning package scikit-learn has a build-in utility function that implements the Hungarian algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGFownwMFQ9E"
      },
      "source": [
        "In the below logic, linear_assignment by default minimizes an objective function. So we need to reverse the sign of IOU_mat for maximization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvrJw_IFXhj"
      },
      "source": [
        "**Unmatched detections and trackers:**\n",
        "\n",
        "\n",
        "Based on the linear assignment results, we keep two lists for unmatched detections and unmatched trackers, respectively. When a car enters into a frame and is first detected, it is not matched with any existing tracks, thus this particular detection is referred to as an unmatched detection, as shown in the following figure. In addition, any matching with an overlap less than iou_thrd signifies the existence of an untracked object. When a car leaves the frame, the previously established track has no more detection to associate with. In this scenario, the track is referred to as unmatched track. Thus, the tracker and the detection associated in the matching are added to the lists of unmatched trackers and unmatched detection, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX3-KhM8GBcs"
      },
      "source": [
        "We include two important design parameters, min_hits and max_age, in the pipeline. The parameter min_hits is the number of consecutive matches needed to establish a track. The parameter max_age is number of consecutive unmatched detections before a track is deleted. Both parameters need to be tuned to improve the tracking and detection performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBA8lLtsFTqN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "from collections import deque\n",
        "#from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
        "\n",
        "\n",
        "# Global variables to be used by funcitons of VideoFileClop\n",
        "frame_count = 0 # frame counter\n",
        "\n",
        "max_age = 4  # no.of consecutive unmatched detection before \n",
        "             # a track is deleted\n",
        "\n",
        "min_hits =1  # no. of consecutive matches needed to establish a track\n",
        "\n",
        "tracker_list =[] # list for trackers\n",
        "# list for track ID\n",
        "track_id_list= deque(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'])\n",
        "\n",
        "debug = True\n",
        "\n",
        "def assign_detections_to_trackers(trackers, detections, iou_thrd = 0.3):\n",
        "    '''\n",
        "    From current list of trackers and new detections, output matched detections,\n",
        "    unmatchted trackers, unmatched detections.\n",
        "    '''    \n",
        "    \n",
        "    IOU_mat= np.zeros((len(trackers),len(detections)),dtype=np.float32)\n",
        "    for t,trk in enumerate(trackers):\n",
        "        #trk = convert_to_cv2bbox(trk) \n",
        "        for d,det in enumerate(detections):\n",
        "         #   det = convert_to_cv2bbox(det)\n",
        "            IOU_mat[t,d] = box_iou2(trk,det) \n",
        "    \n",
        "    # Produces matches       \n",
        "    # Solve the maximizing the sum of IOU assignment problem using the\n",
        "    # Hungarian algorithm (also known as Munkres algorithm)\n",
        "    \n",
        "    matched_idx = linear_assignment(-IOU_mat)\n",
        "    matched_idx = np.asarray(matched_idx)\n",
        "    matched_idx = np.transpose(matched_idx)        \n",
        "\n",
        "    unmatched_trackers, unmatched_detections = [], []\n",
        "    for t,trk in enumerate(trackers):\n",
        "        if(t not in matched_idx[:,0]):\n",
        "            unmatched_trackers.append(t)\n",
        "\n",
        "    for d, det in enumerate(detections):\n",
        "        if(d not in matched_idx[:,1]):\n",
        "            unmatched_detections.append(d)\n",
        "\n",
        "    matches = []\n",
        "   \n",
        "    # For creating trackers we consider any detection with an \n",
        "    # overlap less than iou_thrd to signifiy the existence of \n",
        "    # an untracked object\n",
        "    \n",
        "    for m in matched_idx:\n",
        "        if(IOU_mat[m[0],m[1]]<iou_thrd):\n",
        "            unmatched_trackers.append(m[0])\n",
        "            unmatched_detections.append(m[1])\n",
        "        else:\n",
        "            matches.append(m.reshape(1,2))\n",
        "    \n",
        "    if(len(matches)==0):\n",
        "        matches = np.empty((0,2),dtype=int)\n",
        "    else:\n",
        "        matches = np.concatenate(matches,axis=0)\n",
        "    \n",
        "    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)       \n",
        "    \n",
        "\n",
        "\n",
        "def pipeline(img):\n",
        "    '''\n",
        "    Pipeline function for detection and tracking\n",
        "    '''\n",
        "    global frame_count\n",
        "    global tracker_list\n",
        "    global max_age\n",
        "    global min_hits\n",
        "    global track_id_list\n",
        "    global debug\n",
        "    \n",
        "    frame_count+=1\n",
        "    \n",
        "    img_dim = (img.shape[1], img.shape[0])\n",
        "    z_box = det.get_localization(img) # measurement\n",
        "    if debug:\n",
        "       print('Frame:', frame_count)\n",
        "       \n",
        "    x_box =[]\n",
        "    if debug: \n",
        "        for i in range(len(z_box)):\n",
        "           img1= draw_box_label(img, z_box[i], box_color=(255, 0, 0))\n",
        "           plt.imshow(img1)\n",
        "        plt.show()\n",
        "    \n",
        "    if len(tracker_list) > 0:\n",
        "        for trk in tracker_list:\n",
        "            x_box.append(trk.box)\n",
        "    \n",
        "    \n",
        "    matched, unmatched_dets, unmatched_trks \\\n",
        "    = assign_detections_to_trackers(x_box, z_box, iou_thrd = 0.3)  \n",
        "    if debug:\n",
        "         print('Detection: ', z_box)\n",
        "         print('x_box: ', x_box)\n",
        "         print('matched:', matched)\n",
        "         print('unmatched_det:', unmatched_dets)\n",
        "         print('unmatched_trks:', unmatched_trks)\n",
        "    \n",
        "         \n",
        "    # Deal with matched detections  \n",
        "\n",
        "    # When the car is detected again in the second video frame, running the following assign_detections_to_trackers returns an one-element list , an empty list, \n",
        "    # and an empty list for matched, unmatched_dets, and unmatched_trks, respectively.\n",
        "\n",
        "    if matched.size >0:\n",
        "        for trk_idx, det_idx in matched:\n",
        "            z = z_box[det_idx]\n",
        "            z = np.expand_dims(z, axis=0).T\n",
        "            tmp_trk= tracker_list[trk_idx]\n",
        "            tmp_trk.kalman_filter(z)\n",
        "            xx = tmp_trk.x_state.T[0].tolist()\n",
        "            xx =[xx[0], xx[2], xx[4], xx[6]]\n",
        "            x_box[trk_idx] = xx\n",
        "            tmp_trk.box =xx\n",
        "            tmp_trk.hits += 1\n",
        "            tmp_trk.no_losses = 0\n",
        "    # The baove code block carries out two important tasks, \n",
        "    # 1) carrying out the Kalman filter's prediction and update stages tmp_trk.kalman_filter(); \n",
        "    # 2) increasing the hits of the track by one tmp_trk.hits +=1. With this update, the condition if ((trk.hits >= min_hits) and (trk.no_losses <=max_age)) is statified, \n",
        "    # so the track is fully established. As the result, the bounding box is annotated in the output image\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    # Deal with unmatched detections \n",
        "\n",
        "    # The below code block carries out two important tasks, 1) creating a new tracker tmp_trk for the detection; 2) carrying out the Kalman filter's predict stage tmp_trk.predict_only()\n",
        "\n",
        "    if len(unmatched_dets)>0:\n",
        "        for idx in unmatched_dets:\n",
        "            z = z_box[idx]\n",
        "            z = np.expand_dims(z, axis=0).T\n",
        "            tmp_trk = Tracker() # Create a new tracker\n",
        "            x = np.array([[z[0], 0, z[1], 0, z[2], 0, z[3], 0]]).T\n",
        "            tmp_trk.x_state = x\n",
        "            tmp_trk.predict_only()\n",
        "            xx = tmp_trk.x_state\n",
        "            xx = xx.T[0].tolist()\n",
        "            xx =[xx[0], xx[2], xx[4], xx[6]]\n",
        "            tmp_trk.box = xx\n",
        "            tmp_trk.id = track_id_list.popleft() # assign an ID for the tracker\n",
        "            tracker_list.append(tmp_trk)\n",
        "            x_box.append(xx)\n",
        "\n",
        "  # \n",
        "    \n",
        "    # Deal with unmatched tracks       \n",
        "    if len(unmatched_trks)>0:\n",
        "        for trk_idx in unmatched_trks:\n",
        "            tmp_trk = tracker_list[trk_idx]\n",
        "            tmp_trk.no_losses += 1\n",
        "            tmp_trk.predict_only()\n",
        "            xx = tmp_trk.x_state\n",
        "            xx = xx.T[0].tolist()\n",
        "            xx =[xx[0], xx[2], xx[4], xx[6]]\n",
        "            tmp_trk.box =xx\n",
        "            x_box[trk_idx] = xx\n",
        "                   \n",
        "       \n",
        "    # The list of tracks to be annotated  \n",
        "    good_tracker_list =[]\n",
        "    for trk in tracker_list:\n",
        "        if ((trk.hits >= min_hits) and (trk.no_losses <=max_age)):\n",
        "             good_tracker_list.append(trk)\n",
        "             x_cv2 = trk.box\n",
        "             if debug:\n",
        "                 print('updated box: ', x_cv2)\n",
        "                 print()\n",
        "             img= draw_box_label(img, x_cv2) # Draw the bounding boxes on the \n",
        "                                             # images\n",
        "    # Book keeping\n",
        "    deleted_tracks = filter(lambda x: x.no_losses >max_age, tracker_list)  \n",
        "    \n",
        "    for trk in deleted_tracks:\n",
        "            track_id_list.append(trk.id)\n",
        "    \n",
        "    tracker_list = [x for x in tracker_list if x.no_losses<=max_age]\n",
        "    \n",
        "    if debug:\n",
        "       print('Ending tracker_list: ',len(tracker_list))\n",
        "       print('Ending good tracker_list: ',len(good_tracker_list))\n",
        "    \n",
        "       \n",
        "    return img\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  det = CarDetector()\n",
        "  if debug: # test on a sequence of images\n",
        "        images = [plt.imread(file) for file in glob.glob('/content/drive/MyDrive/Colab Notebooks/KALMAN FILTER PROJECT/Klaman filter images/*.jpg')]\n",
        "        \n",
        "        for i in range(len(images))[0:7]:\n",
        "             image = images[i]\n",
        "             image_box = pipeline(image)   \n",
        "             plt.imshow(image_box)\n",
        "             plt.show()\n",
        "  else:                         # test on a video file.\n",
        "    start=time.time()\n",
        "#    output = 'test_v7.mp4'\n",
        "    clip1 = VideoFileClip(\"project_video.mp4\")          #.subclip(4,49) # The first 8 seconds doesn't have any cars...\n",
        "    clip = clip1.fl_image(pipeline)\n",
        "    clip.write_videofile(output, audio=False)\n",
        "    end  = time.time()\n",
        "    \n",
        "    print(round(end-start, 2), 'Seconds to finish')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From all the above output images, the condition (trk.hits >= min_hits) and (trk.no_losses <=max_age) is staisfied. so the track is fully established. \n",
        "\n",
        "As the result, the bounding box is annotated in the output image, as shown in the above figure."
      ],
      "metadata": {
        "id": "VioySF7c6WtC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Vehicle detection and tracking project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}